# GenAI RAG Application - Environment Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# DATA CONFIGURATION
# =============================================================================

# Path to the dataset file
# For local development: data/arxiv_2.9k.jsonl
# For Docker: /data/arxiv_2.9k.jsonl
DATA_PATH=data/arxiv_2.9k.jsonl

# Directory where the vector index will be stored
# For local development: index
# For Docker: /index
INDEX_DIR=index

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Path to the LLM model file (GGUF format)
# For local development: models/Llama-3.2-3B-Instruct-Q4_K_M.gguf
# For Docker: /models/Llama-3.2-3B-Instruct-Q4_K_M.gguf
MODEL_PATH=models/Llama-3.2-3B-Instruct-Q4_K_M.gguf

# Embedding model from sentence-transformers
# Options: all-mpnet-base-v2 (768-dim), allenai/specter2 (768-dim, scientific papers)
EMBEDDING_MODEL=allenai/specter2

# Number of CPU threads for LLM inference (adjust based on your CPU)
N_THREADS=4

# Context window size (tokens)
N_CTX=16384

# Force CPU usage for all models (embedding and LLM inference)
FORCE_CPU=false
# Number of GPU layers to offload (0 = CPU only, 35 = full GPU)
# When FORCE_CPU=true, set N_GPU_LAYERS=0 automatically
N_GPU_LAYERS=35

# Local cache directory for embedding models (for offline operation)
# All embedding models will be downloaded and stored here
EMBEDDING_CACHE_DIR=models/embeddings

# Force local-only mode for embeddings (no internet downloads)
# Set to true for on-premise/offline operation
# Set to false to allow downloading models from HuggingFace if not cached
EMBEDDING_LOCAL_ONLY=true

# =============================================================================
# IMAGE GENERATION 
# =============================================================================

# Image generation provider options: "local", "pollinations", or "openai"
# local: Defaults to sdxl-turbo model (requires ~2GB VRAM)
# pollinations: Free, no API key needed
# openai: Requires API key from OpenAI Compatible Endpoint
IMAGE_API_PROVIDER=pollinations

# only for IMAGE_API_PROVIDER=local
IMAGE_MODEL_PATH=models/sdxl-turbo
IMAGE_MODEL_NAME=stabilityai/sdxl-turbo

# When using OpenAI compatible endpoint API key
IMAGE_API_KEY=
IMAGE_INFERENCE_STEPS=100
IMAGE_GUIDANCE_SCALE=12.0
# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================

# Default number of documents to retrieve for each query
DEFAULT_TOP_K=7

# Maximum number of documents that can be requested
MAX_TOP_K=20

# Multiplier for reranking (retrieve top_k * multiplier candidates, then rerank to top_k)
RERARK_MULTIPLIER=5

# =============================================================================
# LLM GENERATION SETTINGS (OPTIMIZED FOR BETTER SPECIFICITY)
# =============================================================================

# Maximum tokens for LLM generation
MAX_TOKENS=800

# Temperature for LLM generation (0.0 = deterministic, 2.0 = very creative)
TEMPERATURE=0.2

# Top-p sampling for LLM generation
TOP_P=0.9

# Repeat penalty to reduce repetition in generated text
REPEAT_PENALTY=1.15

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server host (0.0.0.0 = all interfaces, 127.0.0.1 = localhost only)
HOST=127.0.0.1

# Server port (8080 per assignment requirements)
PORT=8080

# Log level: debug, info, warning, error
LOG_LEVEL=info

# Enable CORS (for API access from other origins)
ENABLE_CORS=true

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================

# Batch size for embedding generation
EMBEDDING_BATCH_SIZE=32

# =============================================================================
# ADVANCED OPTIONS
# =============================================================================

# Force rebuild index on startup (true/false)
FORCE_REBUILD=false

# Enable streaming responses (true/false)
ENABLE_STREAMING=true

# Request timeout in seconds
REQUEST_TIMEOUT=120

# Cache embeddings to disk (true/false)
CACHE_EMBEDDINGS=true

# =============================================================================
# LATEX PROCESSING
# =============================================================================

# Enable LaTeX math symbol processing for better academic paper support
LATEX_PROCESSING_ENABLED=true

# =============================================================================
# Sentence Transformers Update Check
# =============================================================================

# Skip checking for sentence-transformers model updates on Hugging Face Hub
# Set to false to check for model updates on every startup
SKIP_CHECK_ST_UPDATES=true


