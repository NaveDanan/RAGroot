# GenAI RAG Application - Local Development Environment Configuration
# Copy this file to .env when running locally
# python main.py

# =============================================================================
# DATA CONFIGURATION
# =============================================================================

# Path to the dataset file (local relative path)
DATA_PATH=data/arxiv_2.9k.jsonl

# Directory where the vector index will be stored (local directory)
INDEX_DIR=index

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Path to the LLM model file (GGUF format) - local path
# Download from: https://huggingface.co/
MODEL_PATH=models/Llama-3.2-3B-Instruct-Q4_K_M.gguf

# Embedding model from sentence-transformers
# Options: all-mpnet-base-v2 (768-dim), allenai/specter2 (768-dim, scientific papers)
# all-MiniLM-L6-v2 (384-dim, faster, smaller)
EMBEDDING_MODEL=allenai/specter2

# Number of CPU threads for LLM inference (adjust based on your CPU cores)
# Recommended: Set to number of physical cores (not hyperthreads)
N_THREADS=8

# Context window size (tokens)
# Larger values use more RAM but handle longer documents
N_CTX=16384

# Force CPU usage for all models (embedding and LLM inference)
# Set to true to disable GPU even if available (useful for testing)
# Set to false to use GPU if available (much faster)
FORCE_CPU=false

# Number of GPU layers to offload (0 = CPU only, 35 = full GPU)
# For 3B models: 35 layers uses ~2-3GB VRAM
# For 7B models: 35 layers uses ~5-6GB VRAM
# Adjust based on your GPU memory
N_GPU_LAYERS=35

# Local cache directory for embedding models (for offline operation)
# All embedding models will be downloaded and stored here
EMBEDDING_CACHE_DIR=models/embeddings

# Force local-only mode for embeddings (no internet downloads)
# Set to true for on-premise/offline operation after models are downloaded
# Set to false to allow automatic downloading from HuggingFace
EMBEDDING_LOCAL_ONLY=true

# =============================================================================
# IMAGE GENERATION 
# =============================================================================

# Image generation provider options: "local", "pollinations", or "openai"
# local: Best quality, requires ~7GB disk space and ~2GB VRAM (sdxl-turbo)
# pollinations: Free, no API key needed, good quality, no local storage
# openai: Highest quality, requires API key and costs money
IMAGE_API_PROVIDER=local

# only for IMAGE_API_PROVIDER=local
# Models will be downloaded automatically to this path if not present
IMAGE_MODEL_PATH=models/sdxl-turbo
IMAGE_MODEL_NAME=stabilityai/sdxl-turbo

# When using OpenAI compatible endpoint API key
# Get your key from: https://platform.openai.com/api-keys
IMAGE_API_KEY=

# Image generation quality settings
# Higher steps = better quality but slower (1-100)
IMAGE_INFERENCE_STEPS=100
# Higher guidance = more adherence to prompt (0.0-20.0)
IMAGE_GUIDANCE_SCALE=12.0

# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================

# Default number of documents to retrieve for each query
# Higher values provide more context but may include less relevant results
DEFAULT_TOP_K=7

# Maximum number of documents that can be requested via API
MAX_TOP_K=20

# Multiplier for reranking (retrieve top_k * multiplier candidates, then rerank to top_k)
# Higher values improve quality but increase computation time
RERANK_MULTIPLIER=5

# =============================================================================
# LLM GENERATION SETTINGS (OPTIMIZED FOR BETTER SPECIFICITY)
# =============================================================================

# Maximum tokens for LLM generation
# Higher values allow longer responses but take more time
MAX_TOKENS=800

# Temperature for LLM generation (0.0 = deterministic, 2.0 = very creative)
# Lower values (0.1-0.3) for factual answers
# Higher values (0.7-1.5) for creative writing
TEMPERATURE=0.2

# Top-p sampling for LLM generation
# Controls diversity (0.9 = balanced, 0.95 = more diverse)
TOP_P=0.9

# Repeat penalty to reduce repetition in generated text
# Higher values (1.1-1.3) reduce repetition
REPEAT_PENALTY=1.15

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server host
# 127.0.0.1 = localhost only (secure, only local access)
# 0.0.0.0 = all interfaces (allows LAN access)
HOST=127.0.0.1

# Server port (8080 per assignment requirements)
# Change if port is already in use
PORT=8080

# Log level: debug, info, warning, error
# Use 'debug' for development, 'info' for production
LOG_LEVEL=info

# Enable CORS (for API access from other origins)
# Set to true for local development with separate frontend
ENABLE_CORS=true

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================

# Batch size for embedding generation
# Higher values faster but use more memory
# GPU: 32-64, CPU: 16-32
EMBEDDING_BATCH_SIZE=32

# =============================================================================
# ADVANCED OPTIONS
# =============================================================================

# Force rebuild index on startup (true/false)
# Set to true to rebuild index from scratch
# Set to false to use cached index (faster startup)
FORCE_REBUILD=false

# Enable streaming responses (true/false)
# Streaming provides real-time tokens as they're generated
ENABLE_STREAMING=true

# Request timeout in seconds
# Increase for slower systems or complex queries
REQUEST_TIMEOUT=120

# Cache embeddings to disk (true/false)
# Speeds up subsequent runs by caching computed embeddings
CACHE_EMBEDDINGS=true

# =============================================================================
# LATEX PROCESSING
# =============================================================================

# Enable LaTeX math symbol processing for better academic paper support
# Converts LaTeX math symbols to readable text
LATEX_PROCESSING_ENABLED=true

# =============================================================================
# Sentence Transformers Update Check
# =============================================================================

# Skip checking for sentence-transformers model updates on Hugging Face Hub
# Set to true to skip version checks and speed up startup
# Set to false to check for updates on every startup
SKIP_CHECK_ST_UPDATES=true

# =============================================================================
# LOCAL DEVELOPMENT QUICK START
# =============================================================================
#
# 1. Copy this file:
#    cp .env.local.example .env
#
# 2. Download models (one-time, requires internet):
#    python tools/download_models.py --all
#
# 3. Validate setup:
#    python tools/validate_offline.py
#
# 4. Build index:
#    python -m utils.indexer
#
# 5. Run application:
#    python main.py
#
# 6. Access web UI:
#    http://localhost:8080
#
# =============================================================================
# SYSTEM REQUIREMENTS
# =============================================================================
#
# Minimum (CPU only):
# - CPU: 4+ cores
# - RAM: 8GB
# - Disk: 10GB free
# - OS: Windows 10/11, Linux, macOS
#
# Recommended (GPU):
# - CPU: 8+ cores
# - RAM: 16GB
# - GPU: NVIDIA with 6GB+ VRAM (for LLM + embeddings)
# - Disk: 15GB free
# - CUDA: 11.8+ or 12.x
#
# Optimal (GPU + Local Image Gen):
# - CPU: 8+ cores
# - RAM: 32GB
# - GPU: NVIDIA with 12GB+ VRAM
# - Disk: 20GB free
# - CUDA: 12.x
