# GenAI RAG Application - Docker Environment Configuration
# Copy this file to .env when running with Docker
# docker run -p 8080:8080 --env-file .env navedanan/genai-app:latest

# =============================================================================
# DATA CONFIGURATION
# =============================================================================

# Path to the dataset file (Docker container paths)
DATA_PATH=/data/arxiv_2.9k.jsonl

# Directory where the vector index will be stored (Docker volume)
INDEX_DIR=/index

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Path to the LLM model file (GGUF format) - Docker container path
MODEL_PATH=/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf

# Embedding model from sentence-transformers
# Options: all-mpnet-base-v2 (768-dim), allenai/specter2 (768-dim, scientific papers)
EMBEDDING_MODEL=allenai/specter2

# Number of CPU threads for LLM inference (adjust based on Docker resource allocation)
N_THREADS=4

# Context window size (tokens)
N_CTX=16384

# Force CPU usage for all models (embedding and LLM inference)
# Set to true if Docker container doesn't have GPU access
FORCE_CPU=true

# Number of GPU layers to offload (0 = CPU only, 35 = full GPU)
# Set to 0 for CPU-only Docker containers
# Set to 20-35 if using nvidia-docker with GPU support
N_GPU_LAYERS=0

# Local cache directory for embedding models (for offline operation)
# All embedding models will be downloaded and stored here
EMBEDDING_CACHE_DIR=/models/embeddings

# Force local-only mode for embeddings (no internet downloads)
# Set to true for on-premise/offline operation
# IMPORTANT: Ensure models are pre-downloaded and included in Docker image
EMBEDDING_LOCAL_ONLY=true

# =============================================================================
# IMAGE GENERATION 
# =============================================================================

# Image generation provider options: "local", "pollinations", or "openai"
    # Recommended for Docker: "pollinations" (no extra model storage needed)
    # local: Requires ~7GB extra in Docker image for sdxl-turbo, GPU recommended
    # pollinations: Free, no API key needed, works with CPU/GPU, requires internet
    # openai: Requires API key from OpenAI Compatible Endpoint, works with CPU/GPU
IMAGE_API_PROVIDER=pollinations

# only for IMAGE_API_PROVIDER=local (Docker container path)
IMAGE_MODEL_PATH=/models/sdxl-turbo
IMAGE_MODEL_NAME=stabilityai/sdxl-turbo

# When using OpenAI compatible endpoint API key
IMAGE_API_KEY=
IMAGE_INFERENCE_STEPS=100
IMAGE_GUIDANCE_SCALE=12.0

# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================

# Default number of documents to retrieve for each query
DEFAULT_TOP_K=7

# Maximum number of documents that can be requested
MAX_TOP_K=20

# Multiplier for reranking (retrieve top_k * multiplier candidates, then rerank to top_k)
RERANK_MULTIPLIER=5

# =============================================================================
# LLM GENERATION SETTINGS (OPTIMIZED FOR BETTER SPECIFICITY)
# =============================================================================

# Maximum tokens for LLM generation
MAX_TOKENS=800

# Temperature for LLM generation (0.0 = deterministic, 2.0 = very creative)
TEMPERATURE=0.2

# Top-p sampling for LLM generation
TOP_P=0.9

# Repeat penalty to reduce repetition in generated text
REPEAT_PENALTY=1.15

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server host (0.0.0.0 = all interfaces - required for Docker)
HOST=0.0.0.0

# Server port (8080 per assignment requirements)
# Must match the EXPOSE port in Dockerfile and -p flag in docker run
PORT=8080

# Log level: debug, info, warning, error
LOG_LEVEL=info

# Enable CORS (required for web UI access)
ENABLE_CORS=true

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================

# Batch size for embedding generation
# Use smaller batches for Docker containers with limited memory
EMBEDDING_BATCH_SIZE=16

# =============================================================================
# ADVANCED OPTIONS
# =============================================================================

# Force rebuild index on startup (true/false)
# Set to false to preserve index across container restarts
FORCE_REBUILD=false

# Enable streaming responses (true/false)
ENABLE_STREAMING=true

# Request timeout in seconds
REQUEST_TIMEOUT=120

# Cache embeddings to disk (true/false)
CACHE_EMBEDDINGS=true

# =============================================================================
# LATEX PROCESSING
# =============================================================================

# Enable LaTeX math symbol processing for better academic paper support
LATEX_PROCESSING_ENABLED=true

# =============================================================================
# Sentence Transformers Update Check
# =============================================================================

# Skip checking for sentence-transformers model updates on Hugging Face Hub
# Set to true for Docker to avoid startup delays
SKIP_CHECK_ST_UPDATES=true

# =============================================================================
# DOCKER DEPLOYMENT NOTES
# =============================================================================
# 
# Example docker run command:
# 
# docker run -d \
#   --name rag-app \
#   -p 8080:8080 \
#   --env-file .env \
#   -v $(pwd)/data:/data:ro \
#   -v $(pwd)/models:/models:ro \
#   -v rag-index:/index \
#   navedanan/genai-app:latest
#
# For GPU support (requires nvidia-docker):
#
# docker run -d \
#   --name rag-app \
#   --gpus all \
#   -p 8080:8080 \
#   --env-file .env \
#   -e FORCE_CPU=false \
#   -e N_GPU_LAYERS=35 \
#   -v $(pwd)/data:/data:ro \
#   -v $(pwd)/models:/models:ro \
#   -v rag-index:/index \
#   navedanan/genai-app:latest
#
# Volume mapping:
# - /data: Dataset files (read-only)
# - /models: Model files (read-only)
# - /index: Vector index (persistent volume for caching)
#
# =============================================================================
